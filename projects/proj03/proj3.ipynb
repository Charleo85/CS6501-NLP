{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<stop>': 0}\n",
    "idx2word = ['<stop>']\n",
    "index = 1\n",
    "max_seq_len = 0\n",
    "\n",
    "def load_voc(filename):\n",
    "    print(\"loading %s\"%filename)\n",
    "    global index, max_seq_len, word2idx, idx2word\n",
    "    sentences = []\n",
    "    num_tokens = 0\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            stn = []\n",
    "            for w in line.rstrip().split(' '):\n",
    "                if w not in word2idx: \n",
    "                    word2idx[w] = index\n",
    "                    index += 1\n",
    "                    idx2word.append(w)\n",
    "                stn.append(word2idx[w])\n",
    "            num_tokens += len(stn)\n",
    "            max_seq_len = max(max_seq_len, len(stn))\n",
    "            sentences.append( np.array(stn, dtype=int) )\n",
    "    print(\"#sentences {}, #tokens {}\".format(len(sentences), num_tokens))\n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trn-wiki.txt\n",
      "#sentences 17556, #tokens 1800340\n",
      "loading dev-wiki.txt\n",
      "#sentences 1841, #tokens 188963\n",
      "loading tst-wiki.txt\n",
      "#sentences 2183, #tokens 212719\n",
      "vocb size 27767\n",
      "max stn len 641\n"
     ]
    }
   ],
   "source": [
    "trn_sentences = load_voc('trn-wiki.txt')\n",
    "dev_sentences = load_voc('dev-wiki.txt')\n",
    "tst_sentences = load_voc('tst-wiki.txt')\n",
    "print('vocb size %d'%index)\n",
    "print('max stn len %d'%max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = index\n",
    "input_size = 32\n",
    "hidden_size = 32\n",
    "batch_size = 32\n",
    "seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordids_placeholder = tf.placeholder(tf.int64, [batch_size, None])\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, input_size], trainable=True)\n",
    "embedded_words = tf.nn.embedding_lookup(word_embeddings, wordids_placeholder)\n",
    "\n",
    "lstm = tf.contrib.cudnn_rnn.CudnnLSTM(2, hidden_size)\n",
    "output2wordid = tf.layers.Dense(vocabulary_size)\n",
    "\n",
    "seq_weight = tf.cast(tf.sign(wordids_placeholder[:,1:]), tf.float32)\n",
    "seq_length = tf.cast(tf.reduce_sum(seq_weight, axis=1), tf.int32)\n",
    "total_seq_length = tf.cast(tf.reduce_sum(seq_length, axis=0), tf.float32)\n",
    "\n",
    "inputs = embedded_words[:, :-1, ]\n",
    "outputs, state = lstm(inputs)\n",
    "\n",
    "labels = wordids_placeholder[:,1:]\n",
    "logits = tf.map_fn(lambda x: output2wordid(x), outputs)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits) * seq_weight\n",
    "\n",
    "# preds = tf.argmax(logits, axis=-1)\n",
    "# acces = tf.cast(tf.equal(preds, labels), tf.float32)\n",
    "    \n",
    "total_loss = tf.reduce_sum(losses) / total_seq_length\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "perplexity = tf.exp(total_loss)\n",
    "tf.summary.scalar('perplexity', perplexity)\n",
    "\n",
    "# accuracy = tf.reduce_mean(acces)\n",
    "# tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#     train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads_and_vars = opt.compute_gradients(total_loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
    "    train_step = opt.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638b7b79c1704115b86ccd0dda77ccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 0', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683.7567255909507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf8a50c3097432c935e307d06b00fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 0', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166.9886299305033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed97717c2b5d403299fc8f5a2d1e1584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 1', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953.9654510103248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ed465bfb7748f3bfcfa500689c6d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 1', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797.9252775561479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a04a24d1f5c48f580383dec5271638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 2', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697.5038325918881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3fb3987ad841ef9ba8d66152896423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 2', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654.2010072327398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d45a02801c451f9f1c725ea2b1f5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 3', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593.9921129113538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1e456953464d5790010adf175a2665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 3', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586.1735562495693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb02b6ad4cb4eeeb3b27fe9395db0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 4', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540.5446447012463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224263f264ee4c37a8d8adca0577e6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 4', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548.0394358548497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88749a3384524c0bb5a43228346dd337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 5', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.1542941021347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269cb35ca6cf4f618175aaf091567553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 5', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520.9864541289346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b2478c1914d299e0755203807f7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 6', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472.2925925346374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d84c043b917481a9d5f40173960b25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 6', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498.5720407526523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116ce5b0370b44ebb8a54e522ef4fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 7', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450.6047973757119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce992d84b074673b881da6629cf0274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 7', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494.97375416641444\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b916fb35778d4662935e90c06c290e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 8', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432.6919408281369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c619d71a9c4df8bed2c13bb2bfd615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 8', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483.11216665277334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc106ac006406da83de86f5e4bdf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 9', max=547, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417.4465160015626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4191a3e6b67432c801778cbd3e445ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='validate epoch 9', max=56, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472.9016711701382\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "epoches = 10\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    now = time.strftime(\"%c\")\n",
    "    train_writer = tf.summary.FileWriter('./logs/'+now, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_idx in range(epoches):\n",
    "        num_batch = len(trn_sentences) // batch_size - 1\n",
    "        loss_arr = []\n",
    "        weight_arr = []\n",
    "        for batch_id in tnrange(num_batch, desc='training epoch %d'%epoch_idx):\n",
    "            len_arr = [sent.shape[0] for sent in trn_sentences[batch_id*batch_size:(batch_id+1)*batch_size]]\n",
    "            max_len = max(len_arr)\n",
    "            total_len = sum(len_arr)\n",
    "            \n",
    "            padded = [np.pad( sent, (0,  max_len - sent.shape[0]), 'edge') for sent in trn_sentences[batch_id*batch_size:(batch_id+1)*batch_size] ]\n",
    "            batch_stn = np.stack(padded, axis=0)\n",
    "            summary_, _, total_loss_ = sess.run(\n",
    "                [merged_summary, train_step, total_loss], \n",
    "                feed_dict = {\n",
    "                    learning_rate : 0.01,\n",
    "                    wordids_placeholder: batch_stn\n",
    "                })              \n",
    "            train_writer.add_summary(summary_, num_batch*epoch_idx+batch_id)\n",
    "            loss_arr.append(total_loss_)\n",
    "            weight_arr.append(total_len)\n",
    "        print( np.exp(np.average(loss_arr, weights=weight_arr)) )\n",
    "        \n",
    "        loss_arr = []\n",
    "        weight_arr = []\n",
    "        # eval\n",
    "        num_batch = len(dev_sentences) // batch_size - 1\n",
    "        for batch_id in tnrange(num_batch, desc='validate epoch %d'%epoch_idx):\n",
    "            len_arr = [sent.shape[0] for sent in trn_sentences[batch_id*batch_size:(batch_id+1)*batch_size]]\n",
    "            max_len = max(len_arr)\n",
    "            total_len = sum(len_arr)\n",
    "            \n",
    "            padded = [np.pad( sent, (0,  max_len - sent.shape[0]), 'edge') for sent in trn_sentences[batch_id*batch_size:(batch_id+1)*batch_size] ]\n",
    "            batch_stn = np.stack(padded, axis=0)\n",
    "            total_loss_ = sess.run(\n",
    "                total_loss, \n",
    "                feed_dict = {\n",
    "                    wordids_placeholder: batch_stn\n",
    "                })  \n",
    "            loss_arr.append(total_loss_)\n",
    "            weight_arr.append(total_len)\n",
    "        print( np.exp(np.average(loss_arr, weights=weight_arr)) )\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordids_placeholder = tf.placeholder(tf.int64, [batch_size, seq_len])\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, input_size], trainable=True)\n",
    "embedded_words = tf.nn.embedding_lookup(word_embeddings, wordids_placeholder)\n",
    "\n",
    "lstm = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output2wordid = tf.layers.Dense(vocabulary_size)\n",
    "\n",
    "seq_weight = tf.sign(wordids_placeholder[:,1:])\n",
    "seq_length = tf.cast(tf.reduce_sum(seq_weight, axis=1), tf.int32)\n",
    "\n",
    "outputs, state = tf.nn.dynamic_rnn(lstm, \n",
    "                                   embedded_words[:, :-1, ], \n",
    "                                   sequence_length=seq_length, \n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "num_pred = tf.squeeze(seq_length)\n",
    "labels = wordids_placeholder[:,1:num_pred+1]\n",
    "logits = tf.map_fn(lambda x: output2wordid(x), outputs)[:,:num_pred]\n",
    "\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "\n",
    "preds = tf.argmax(logits, axis=-1)\n",
    "acces = tf.cast(tf.equal(preds, labels), tf.float32)\n",
    "probs = tf.log(tf.reduce_max(logits, axis=-1) / tf.reduce_sum(logits, axis=-1))\n",
    "\n",
    "\n",
    "# initial_c_state = tf.get_variable(\"initial_c_hidden_state\", [batch_size, hidden_size], initializer= tf.initializers.random_uniform, trainable=True )\n",
    "# initial_m_state = tf.get_variable(\"initial_m_hidden_state\", [batch_size, hidden_size], initializer= tf.initializers.random_uniform, trainable=True )\n",
    "\n",
    "# embedded_word_series = tf.unstack(embedded_words, axis=1)\n",
    "# # print(embedded_word_series)\n",
    "# state = (initial_c_state, initial_m_state)\n",
    "# losses = []\n",
    "# acc = []\n",
    "# for i in range(len(embedded_word_series)-1):\n",
    "#     embedded_word = embedded_word_series[i]\n",
    "#     output, state = lstm(embedded_word, state)\n",
    "    \n",
    "#     correct_pred = wordids_placeholder[:, i+1]\n",
    "    \n",
    "#     prob = output2wordid(output)\n",
    "#     acc.append( tf.cast(tf.equal(tf.argmax(prob, axis=1), correct_pred), tf.float32) )\n",
    "# #     print(prob.shape, correct_pred.shape)\n",
    "#     losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prob, labels = correct_pred))\n",
    "    \n",
    "    \n",
    "total_loss = tf.reduce_mean(losses)\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "perplexity = tf.exp(total_loss)\n",
    "tf.summary.scalar('perplexity', perplexity)\n",
    "\n",
    "accuracy = tf.reduce_mean(acces)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#     train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = opt.compute_gradients(total_loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
    "    train_step = opt.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "epoches = 10\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    now = time.strftime(\"%c\")\n",
    "    train_writer = tf.summary.FileWriter('./logs/'+now, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_idx in range(epoches):\n",
    "        num_batch = len(trn_sentences) / batch_size\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(trn_sentences, desc='training epoch %d'%epoch_idx)):\n",
    "            stn_len = stn.shape[0]\n",
    "            i = 0\n",
    "            input_stn = np.expand_dims(np.pad( stn[i:i+seq_len], (0,  i+seq_len - stn_len), 'edge'), axis=0)\n",
    "            summary_, _ = sess.run(\n",
    "                [merged_summary, train_step], \n",
    "                feed_dict = {\n",
    "                    learning_rate : 0.01,\n",
    "                    wordids_placeholder: input_stn\n",
    "                })              \n",
    "            train_writer.add_summary(summary_, num_batch*epoch_idx+batch_id)\n",
    "\n",
    "    \n",
    "        loss_arr = []\n",
    "        # eval\n",
    "        num_batch = len(dev_sentences) / batch_size\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(dev_sentences, desc='validate epoch %d'%epoch_idx)):\n",
    "            stn_len = stn.shape[0]\n",
    "            i = 0\n",
    "            input_stn = np.expand_dims(np.pad( stn[i:i+seq_len], (0,  i+seq_len - stn_len), 'edge'), axis=0)\n",
    "            loss_ = sess.run(\n",
    "                total_loss, \n",
    "                feed_dict = {wordids_placeholder: input_stn})  \n",
    "            loss_arr.append(loss_)\n",
    "            \n",
    "        print( np.exp(np.mean(loss_arr)) )\n",
    "    train_writer.close()\n",
    "\n",
    "    # test\n",
    "    f = open('jw7jb-tst-logprob.txt', 'w')\n",
    "    for batch_id, stn in enumerate(tqdm_notebook(tst_sentences, desc='testing')):\n",
    "        input_stn = np.expand_dims(np.pad( stn, (0,  seq_len - stn.shape[0]), 'edge'), axis=0)\n",
    "        probs_ = sess.run(\n",
    "            probs, \n",
    "            feed_dict = {wordids_placeholder: input_stn})  \n",
    "#         f.write('<start> ')\n",
    "        for wid, prob in zip(stn[1:], probs_[0]) :\n",
    "            f.write( '{}\\t{}\\n'.format(idx2word[wid], prob) )\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
