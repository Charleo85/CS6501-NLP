{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'<stop>': 0}\n",
    "idx2word = ['<stop>']\n",
    "index = 1\n",
    "max_seq_len = 0\n",
    "\n",
    "def load_voc(filename):\n",
    "    print(\"loading %s\"%filename)\n",
    "    global index, max_seq_len, word2idx, idx2word\n",
    "    sentences = []\n",
    "    num_tokens = 0\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            stn = []\n",
    "            for w in line.rstrip().split(' '):\n",
    "                if w not in word2idx: \n",
    "                    word2idx[w] = index\n",
    "                    index += 1\n",
    "                    idx2word.append(w)\n",
    "                stn.append(word2idx[w])\n",
    "            num_tokens += len(stn)\n",
    "            max_seq_len = max(max_seq_len, len(stn))\n",
    "            sentences.append( np.array(stn, dtype=int) )\n",
    "    print(\"#sentences {}, #tokens {}\".format(len(sentences), num_tokens))\n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trn-wiki.txt\n",
      "#sentences 17556, #tokens 1800340\n",
      "loading dev-wiki.txt\n",
      "#sentences 1841, #tokens 188963\n",
      "loading tst-wiki.txt\n",
      "#sentences 2183, #tokens 212719\n",
      "vocb size 27767\n",
      "max stn len 641\n"
     ]
    }
   ],
   "source": [
    "trn_sentences = load_voc('trn-wiki.txt')\n",
    "dev_sentences = load_voc('dev-wiki.txt')\n",
    "tst_sentences = load_voc('tst-wiki.txt')\n",
    "print('vocb size %d'%index)\n",
    "print('max stn len %d'%max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = index\n",
    "input_size = 32\n",
    "hidden_size = 32\n",
    "batch_size = 1\n",
    "seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordids_placeholder = tf.placeholder(tf.int64, [1, None])\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, input_size], trainable=True)\n",
    "embedded_words = tf.nn.embedding_lookup(word_embeddings, wordids_placeholder)\n",
    "\n",
    "lstm = tf.contrib.cudnn_rnn.CudnnLSTM(2, hidden_size)\n",
    "output2wordid = tf.layers.Dense(vocabulary_size)\n",
    "\n",
    "inputs = embedded_words[:, :-1, ]\n",
    "outputs, state = lstm(inputs)\n",
    "\n",
    "labels = wordids_placeholder[:,1:]\n",
    "logits = tf.map_fn(lambda x: output2wordid(x), outputs)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "\n",
    "preds = tf.argmax(logits, axis=-1)\n",
    "acces = tf.cast(tf.equal(preds, labels), tf.float32)\n",
    "# probs = tf.log(tf.reduce_max(logits, axis=-1) / tf.reduce_sum(logits, axis=-1))\n",
    "    \n",
    "total_loss = tf.reduce_mean(losses)\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "perplexity = tf.exp(total_loss)\n",
    "tf.summary.scalar('perplexity', perplexity)\n",
    "\n",
    "accuracy = tf.reduce_mean(acces)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#     train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads_and_vars = opt.compute_gradients(total_loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
    "    train_step = opt.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d932b455cf45a28de2092ca284dae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training epoch 0', max=17556, style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198.746253682623\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-191f698933df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mperplexity_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mperplexity_\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_perplexity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_batch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36madd_summary\u001b[0;34m(self, summary, global_step)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# to save space - we just store the metadata on the first value with a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# specific tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "epoches = 10\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    now = time.strftime(\"%c\")\n",
    "    train_writer = tf.summary.FileWriter('./logs/'+now, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_idx in range(epoches):\n",
    "        num_batch = len(trn_sentences) / batch_size\n",
    "        loss_arr = []\n",
    "        weight_arr = []\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(trn_sentences, desc='training epoch %d'%epoch_idx)):\n",
    "            summary_, _ = sess.run(\n",
    "                [merged_summary, train_step], \n",
    "                feed_dict = {\n",
    "                    learning_rate : 0.1,\n",
    "                    wordids_placeholder: np.expand_dims(stn, 0)\n",
    "                })              \n",
    "            train_writer.add_summary(summary_, num_batch*epoch_idx+batch_id)\n",
    "            loss_arr.append(total_loss_)\n",
    "            weight_arr.append(stn.shape[0]-1)\n",
    "            break\n",
    "        print( np.exp(np.average(loss_arr, weights=weight_arr)) )\n",
    "        \n",
    "        loss_arr = []\n",
    "        weight_arr = []\n",
    "        # eval\n",
    "        num_batch = len(dev_sentences) / batch_size\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(dev_sentences, desc='validate epoch %d'%epoch_idx)):\n",
    "            total_loss_ = sess.run(\n",
    "                total_loss, \n",
    "                feed_dict = {\n",
    "                    wordids_placeholder: np.expand_dims(stn, 0)\n",
    "                })  \n",
    "            loss_arr.append(total_loss_)\n",
    "            weight_arr.append(stn.shape[0]-1)\n",
    "        print( np.exp(np.average(loss_arr, weights=weight_arr)) )\n",
    "        \n",
    "    # test\n",
    "    f = open('jw7jb-tst-logprob.txt', 'w')\n",
    "    for batch_id, stn in enumerate(tqdm_notebook(tst_sentences, desc='testing')):\n",
    "        losses_ = sess.run(\n",
    "            losses, \n",
    "            feed_dict = {\n",
    "                wordids_placeholder: np.expand_dims(stn, 0)\n",
    "            })  \n",
    "        for wid, prob in zip(stn[1:], losses_[0]) :\n",
    "            f.write( '{}\\t{}\\n'.format(idx2word[wid], -prob) )\n",
    "    f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordids_placeholder = tf.placeholder(tf.int64, [batch_size, seq_len])\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, input_size], trainable=True)\n",
    "embedded_words = tf.nn.embedding_lookup(word_embeddings, wordids_placeholder)\n",
    "\n",
    "lstm = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output2wordid = tf.layers.Dense(vocabulary_size)\n",
    "\n",
    "seq_weight = tf.sign(wordids_placeholder[:,1:])\n",
    "seq_length = tf.cast(tf.reduce_sum(seq_weight, axis=1), tf.int32)\n",
    "\n",
    "outputs, state = tf.nn.dynamic_rnn(lstm, \n",
    "                                   embedded_words[:, :-1, ], \n",
    "                                   sequence_length=seq_length, \n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "num_pred = tf.squeeze(seq_length)\n",
    "labels = wordids_placeholder[:,1:num_pred+1]\n",
    "logits = tf.map_fn(lambda x: output2wordid(x), outputs)[:,:num_pred]\n",
    "\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "\n",
    "preds = tf.argmax(logits, axis=-1)\n",
    "acces = tf.cast(tf.equal(preds, labels), tf.float32)\n",
    "probs = tf.log(tf.reduce_max(logits, axis=-1) / tf.reduce_sum(logits, axis=-1))\n",
    "\n",
    "\n",
    "# initial_c_state = tf.get_variable(\"initial_c_hidden_state\", [batch_size, hidden_size], initializer= tf.initializers.random_uniform, trainable=True )\n",
    "# initial_m_state = tf.get_variable(\"initial_m_hidden_state\", [batch_size, hidden_size], initializer= tf.initializers.random_uniform, trainable=True )\n",
    "\n",
    "# embedded_word_series = tf.unstack(embedded_words, axis=1)\n",
    "# # print(embedded_word_series)\n",
    "# state = (initial_c_state, initial_m_state)\n",
    "# losses = []\n",
    "# acc = []\n",
    "# for i in range(len(embedded_word_series)-1):\n",
    "#     embedded_word = embedded_word_series[i]\n",
    "#     output, state = lstm(embedded_word, state)\n",
    "    \n",
    "#     correct_pred = wordids_placeholder[:, i+1]\n",
    "    \n",
    "#     prob = output2wordid(output)\n",
    "#     acc.append( tf.cast(tf.equal(tf.argmax(prob, axis=1), correct_pred), tf.float32) )\n",
    "# #     print(prob.shape, correct_pred.shape)\n",
    "#     losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prob, labels = correct_pred))\n",
    "    \n",
    "    \n",
    "total_loss = tf.reduce_mean(losses)\n",
    "tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "perplexity = tf.exp(total_loss)\n",
    "tf.summary.scalar('perplexity', perplexity)\n",
    "\n",
    "accuracy = tf.reduce_mean(acces)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#     train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = opt.compute_gradients(total_loss)\n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in grads_and_vars]\n",
    "    train_step = opt.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "epoches = 10\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    now = time.strftime(\"%c\")\n",
    "    train_writer = tf.summary.FileWriter('./logs/'+now, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_idx in range(epoches):\n",
    "        num_batch = len(trn_sentences) / batch_size\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(trn_sentences, desc='training epoch %d'%epoch_idx)):\n",
    "            stn_len = stn.shape[0]\n",
    "            i = 0\n",
    "            input_stn = np.expand_dims(np.pad( stn[i:i+seq_len], (0,  i+seq_len - stn_len), 'edge'), axis=0)\n",
    "            summary_, _ = sess.run(\n",
    "                [merged_summary, train_step], \n",
    "                feed_dict = {\n",
    "                    learning_rate : 0.01,\n",
    "                    wordids_placeholder: input_stn\n",
    "                })              \n",
    "            train_writer.add_summary(summary_, num_batch*epoch_idx+batch_id)\n",
    "\n",
    "    \n",
    "        loss_arr = []\n",
    "        # eval\n",
    "        num_batch = len(dev_sentences) / batch_size\n",
    "        for batch_id, stn in enumerate(tqdm_notebook(dev_sentences, desc='validate epoch %d'%epoch_idx)):\n",
    "            stn_len = stn.shape[0]\n",
    "            i = 0\n",
    "            input_stn = np.expand_dims(np.pad( stn[i:i+seq_len], (0,  i+seq_len - stn_len), 'edge'), axis=0)\n",
    "            loss_ = sess.run(\n",
    "                total_loss, \n",
    "                feed_dict = {wordids_placeholder: input_stn})  \n",
    "            loss_arr.append(loss_)\n",
    "            \n",
    "        print( np.exp(np.mean(loss_arr)) )\n",
    "    train_writer.close()\n",
    "\n",
    "    # test\n",
    "    f = open('jw7jb-tst-logprob.txt', 'w')\n",
    "    for batch_id, stn in enumerate(tqdm_notebook(tst_sentences, desc='testing')):\n",
    "        input_stn = np.expand_dims(np.pad( stn, (0,  seq_len - stn.shape[0]), 'edge'), axis=0)\n",
    "        probs_ = sess.run(\n",
    "            probs, \n",
    "            feed_dict = {wordids_placeholder: input_stn})  \n",
    "#         f.write('<start> ')\n",
    "        for wid, prob in zip(stn[1:], probs_[0]) :\n",
    "            f.write( '{}\\t{}\\n'.format(idx2word[wid], prob) )\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
