# Schedule

**Subject to change**

| Date | Topics | Readings |
|:-------------|:-----|:---- |
| Aug. 29 | Introduction | |
| Sept. 3 | [Text classification, bag-of-words representations](slides/lecture-02.pdf) | JE Sec. 2.2, 4.4|
| Sept. 5 | [Logistic regression, Regularization](slides/lecture-03.pdf) | JE Sec. 2.4 - 2.6|
| Sept. 10 | [Statistical Language Modeling, n-gram models](slides/lecture-04.pdf) | JE Sec. 6.1, 6.4 - 6.5|
| Sept. 12 | [Sequence labeling (I): Part-of-speech tagging, HMMs](slides/lecture-05.pdf) | JE Sec. 7.1 - 7.2, 8.1|
| Sept. 17 | [Sequence labeling (II): Discriminative models, CRF](slides/lecture-06.pdf) | [Note from M. Collins](http://www.cs.columbia.edu/~mcollins/crf.pdf) |
| Sept. 19 | [Parsing (I): (Probabilistic) Context-free grammar](slides/lecture-07.pdf) | [Note from M. Collins](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf)|
| Sept. 24 | [Parsing (II): Probabilistic Context-free Grammars, CKY algorithm](slides/lecture-08.pdf) | |
| Sept. 26 | [Parsing (III): Dependency grammar and transition based parsing](slides/lecture-09.pdf) | [JM 13.1 - 13.4](https://web.stanford.edu/%7Ejurafsky/slp3/13.pdf) |
| Oct. 1 | [Statistical machine translation](slides/lecture-10.pdf) | [Note from M. Collins](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) Sec. 1 - 5 |
| Oct. 3 | [Feed-forward neural networks and back propagation algorithm](slides/lecture-11.pdf) | [Note from M. Collins](http://www.cs.columbia.edu/~mcollins/ff.pdf) |
| *Oct. 5* | *Project proposal deadline* | |
| Oct. 8 | *Reading day*  | |
| Oct. 10 | **Project proposal presentation** | |
| Oct. 15 | **Project proposal presentation** | |
| Oct. 17 | [Optimization for Deep Learning](slides/lecture-12.pdf) | [Readings Sec. 1](readings.md) |
| Oct. 22 | [Word embeddings (I): Models](slides/lecture-13.pdf)  | [Readings Sec. 2](readings.md) |
| Oct. 24 | [Word embeddings (II): Evaluation and problems](slides/lecture-14.pdf)  | [Readings Sec. 3](readings.md) |
| Oct. 29 | [Recurrent neural network language modeling](slides/lecture-15.pdf) | [Readings Sec. 4](readings.md) |
| Oct. 31 | [Sequence-to-sequence models and attention mechanism](slides/lecture-16.pdf) | [Readings Sec. 5](readings.md) |
| Nov. 5 | Deep latent variable models (I) | [Readings Sec. 6](readings.md) |
| Nov. 7 | Deep latent variable models (II) | [Readings Sec. 7](readings.md) |
| Nov. 12 | Neural text generation | [Readings Sec. 8](readings.md) |
| Nov. 14 | Convolutional neural networks for NLP | |
| Nov. 19 | Representation learning on Texts | |
| Nov. 21 | *Thanksgiving* | |
| Nov. 26 | Generative adversarial networks| |
| Nov. 28 | Reinforcement learning for NLP  | |
| Dec. 3 | Interpretability in NLP| |
| Dec. 5 | **Project presentation** | |
