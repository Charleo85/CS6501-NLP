# Readings

## 1. Optimization for Deep Learning

**Mandatory**

1. Bottou, [Stochastic Gradient Descent Tricks](readings/bottou2012stochastic.pdf), 2012
2. Kingma and Ba, [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf), 2015

**Optional**

- Reddi et al., [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ), 2018
- Masters and Luschi, [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018

## 2. Word Embeddings

**Mandatory**

- Mikolov et al., [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013
- Pennington et al., [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf), 2014
- Faruqui et al., [Retrofitting Word Vectors to Semantic Lexicons](https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf), 2015

**Optional**

- Mikolov et al., [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N13-1090), 2013
- Baroni et al., [Donâ€™t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors](http://www.aclweb.org/anthology/P14-1023), 2014
- Tsvetkov et al., [Evaluation of Word Vector Representations by Subspace Alignment](http://www.aclweb.org/anthology/D15-1243), 2015
