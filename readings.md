# Readings

## 1. Optimization for Deep Learning

**Mandatory**

1. Bottou, [Stochastic Gradient Descent Tricks](readings/bottou2012stochastic.pdf), 2012
2. Kingma and Ba, [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf), 2015

**Optional**

- Reddi et al., [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ), 2018
- Masters and Luschi, [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018

## 2. Word Embeddings: Models

**Mandatory**

- Mikolov et al., [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 2013
- Pennington et al., [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf), 2014
- Faruqui et al., [Retrofitting Word Vectors to Semantic Lexicons](https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf), 2015

**Optional**

- Mikolov et al., [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N13-1090), 2013
- Baroni et al., [Donâ€™t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors](http://www.aclweb.org/anthology/P14-1023), 2014
- Tsvetkov et al., [Evaluation of Word Vector Representations by Subspace Alignment](http://www.aclweb.org/anthology/D15-1243), 2015

## 3. Word Embeddings: Evaluation and Problems

**Mandatory**

- Faruqui et al., [Problems With Evaluation of Word Embeddings Using Word Similarity Tasks](http://www.aclweb.org/anthology/W16-2506), 2016
- Bolukbasi1 et al., [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf), 2016
- Park et al., [Rotated Word Vector Representations and their Interpretability](http://aclweb.org/anthology/D17-1041), 2017


**Optional**

- Subramanian et al., [SPINE: SParse Interpretable Neural Embeddings](https://arxiv.org/abs/1711.08792), 2018
- Schnabel et al., [Evaluation methods for unsupervised word embeddings](https://www.cs.cornell.edu/~schnabts/downloads/schnabel2015embeddings.pdf), 2015
- Bakarov, [A Survey of Word Embeddings Evaluation Methods](https://arxiv.org/abs/1801.09536), 2018
- Brunet et al., [Understanding the Origins of Bias in Word Embeddings](https://arxiv.org/abs/1810.03611), 2018

## 4. RNNs and RNN LMs

**Mandatory**

- Hochreiter and Schmidhuber, [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf), 1997
- Mikolov et al., [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), 2010
- Pascanu et al., [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063.pdf), 2013

**Optional**

- Elman, [Finding Structure in Time](https://pdfs.semanticscholar.org/8633/f28b8aeefdcad94a4a55ccb1abdd18490237.pdf), 1990
- Schuster and Paliwal, [Bidirectional Recurrent Neural Networks](https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf?_ga=2.84099545.110351407.1540518927-965287580.1507685405), 1997
- Bengio et al., [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003
- Sundermeyer et al., [LSTM Neural Networks for Language Modeling](https://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.pdf?_ga=2.174653794.110351407.1540518927-965287580.1507685405), 2012
- Greff et al., [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf), 2017

## 5. Seq2seq Models

**Mandatory**

- Sutskever et al., [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), 2014
- Bahdanau et al., [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), 2015
- Ji et al., [A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models](https://arxiv.org/abs/1603.01913), 2016


**Optional**

- Kim et al., [Structured Attention Networks](https://arxiv.org/pdf/1702.00887.pdf), 2017
- Vinyals et al., [Grammar as a Foreign Language](https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf), 2015
- Rush et al., [A Neural Attention Model for Sentence Summarization](https://www.aclweb.org/anthology/D/D15/D15-1044.pdf), 2015

## 6. Latent Variable Models: Variational Inference and Topic Modeling

**Mandatory**

- Blei et al., [Variational Inference: A Review for Statisticians](https://arxiv.org/pdf/1601.00670.pdf), 2016


**Optional**

- Blei et al., [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), 2003
- Blei and Lafferty, [Topic Models](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf), 2009

## 7. Latent Variable Models: Variational auto-encoder

**Mandatory**

- Kingma and Willing, [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf), 2014

**Optional**

- Rezende et al., [Stochastic backpropagation and approximate inference in deep generative models](https://arxiv.org/pdf/1401.4082.pdf), 2014
- Miao et al., [Neural Variational Inference for Text Processing](https://arxiv.org/pdf/1511.06038.pdf), 2016

## 8. Neural Text Generation

**Mandatory**

- Graves, [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf), 2013
- Bowman et al., [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349), 2016
- Clark et al., [Neural Text Generation in Stories using Entity Representations as Context](http://yangfengji.net/publication/papers/clark2018neural.pdf), 2018

**Optional**

- Sordoni et al., [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](http://yangfengji.net/publication/papers/sordoni-naacl-2015.pdf), 2015
- Gatt and Krahmer, [Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation](https://arxiv.org/pdf/1703.09902.pdf), 2017


