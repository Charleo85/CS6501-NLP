# Readings

## 1. Optimization for Deep Learning

**Mandatory**

1. Bottou, [Stochastic Gradient Descent Tricks](readings/bottou2012stochastic.pdf), 2012
2. Kingma and Ba, [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf), 2015

**Optional**

- Reddi et al., [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ), 2018
- Masters and Luschi, [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018

